(window.webpackJsonp=window.webpackJsonp||[]).push([[31],{510:function(n,t,e){"use strict";e.r(t);var a=e(15),o=Object(a.a)({},(function(){var n=this,t=n.$createElement,e=n._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[e("p",[n._v("选自IEEE信号处理杂志（IEEE SIgnal ProcESSIng MagazInE   |   January 2018   ）")]),n._v(" "),e("p",[e("a",{attrs:{href:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/PDF/Advanced%20deep-learning%20techniques%20for%20salient%20and%20category-specific%20object%20detection-a%20survey.pdf",target:"_blank",rel:"noopener noreferrer"}},[e("font",{attrs:{color:"red"}},[n._v("Advanced Deep-Learning Techniques  for Salient and Category-Specific Object Detection")]),e("OutboundLink")],1)]),n._v(" "),e("p",[n._v("​")]),n._v(" "),e("p",[n._v("目标检测，包括"),e("font",{attrs:{color:"red"}},[n._v("目标性检测（objectness detection，OD）、显著目标检测（ salient object detection，SOD）和特定类别目标检测（category-specific object detection，COD）")]),n._v("，是计算机视觉领域最基本但最具挑战性的问题之一。在过去的几十年里，研究人员为解决这个问题做出了巨大的努力，因为它在其他计算机视觉任务中有着广泛的应用，如"),e("font",{attrs:{color:"red"}},[n._v("活动或事件识别、基于内容的图像检索和场景理解")]),n._v("等。")],1),n._v(" "),e("p",[n._v("尽管近年来出现了许多方法，目前仍缺乏对所提出的高质量目标检测技术，尤其是基于高级深度学习技术的目标检测技术的全面综述。")]),n._v(" "),e("p",[n._v("为此，本文深入探讨了这一研究领域的最新进展，包括")]),n._v(" "),e("p",[e("font",{attrs:{color:"red"}},[n._v("1）每个子方向的定义、动机和任务；")])],1),n._v(" "),e("p",[e("font",{attrs:{color:"red"}},[n._v("2） 现代技术和基本研究趋势；")])],1),n._v(" "),e("p",[e("font",{attrs:{color:"red"}},[n._v("3） 基准数据集和评估指标；")])],1),n._v(" "),e("p",[e("font",{attrs:{color:"red"}},[n._v("4）实验结果的比较与分析。")])],1),n._v(" "),e("p",[n._v("更重要的是，我们将"),e("font",{attrs:{color:"red"}},[n._v("揭示OD、SOD和COD之间的潜在关系")]),n._v("，并详细讨论一些开放性问题，同时指出几个"),e("font",{attrs:{color:"red"}},[n._v("尚未解决的挑战和有前途的未来工作")]),n._v("。")],1),n._v(" "),e("iframe",{attrs:{src:n.$withBase("/mindmap/01.html"),width:"100%",height:"700"}}),n._v(" "),e("h2",{attrs:{id:"引言"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#引言"}},[n._v("#")]),n._v(" 引言")]),n._v(" "),e("p",[n._v("​\t\t作为一项具有挑战性但有用的计算机视觉任务，目标检测旨在识别每个给定图像或视频中存在的各种"),e("font",{attrs:{color:"red"}},[n._v("单独的目标")]),n._v("。在这一研究领域，当处理具有"),e("font",{attrs:{color:"red"}},[n._v("相对简单的图像场景和清晰前景")]),n._v("对象的图像时，已经取得了比较好的结果。然而，当处理包含以"),e("font",{attrs:{color:"red"}},[n._v("任意姿势放置、形状各异且出现在杂乱和封闭环境")]),n._v("中的对象的图像和视频时，这个问题没有得到充分解决。")],1),n._v(" "),e("p",[n._v("​\t\t过去几十年中发表的目标检测研究工作大致可分为三个方向："),e("font",{attrs:{color:"red"}},[n._v("OD、SOD和COD")]),n._v("。具体而言，OD[1]、[2]旨在"),e("font",{attrs:{color:"red"}},[n._v("检测每个给定图像中出现的所有可能对象，而不管具体的对象类别")]),n._v("。它有很大的挑战，因为不同的物体，无论是在同一个物体类别内，还是来自不同的物体类别，都可能有巨大的外观变化，由于其内部固有特征（例如，猫等生物通常比车辆等人造物体具有更多可变形的外观）或外部捕获条件（例如观察距离或角度）（例如，可变形物体在一定距离内可能看起来有些僵硬，甚至僵硬物体在不同视角下也可能表现出变化）。通常，"),e("font",{attrs:{color:"red"}},[n._v("OD算法输出数千个对象建议或假设")]),n._v("，如图1（a）所示，这有利于广泛的计算机视觉任务，如"),e("font",{attrs:{color:"red"}},[n._v("弱监督学习")]),n._v("[3]和"),e("font",{attrs:{color:"red"}},[n._v("对象跟踪")]),n._v("[4]。")],1),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209161116472.png",alt:"image-20220209161116472"}})]),n._v(" "),e("center",[n._v("图1.目标检测的三个研究方向：（a）OD，（b）SOD和（c）COD。")]),n._v(" "),e("p",[n._v("​\t\t"),e("font",{attrs:{color:"red"}},[n._v("SOD")]),n._v("[5]，[6]是目标检测的另一个方向，其目的是"),e("font",{attrs:{color:"red"}},[n._v("模仿视觉注意机制")]),n._v("，突出显示每个给定图像中吸引我们注意力的对象[91]。这是受到人类视觉注意系统的启发，该系统可以引导人类特别注意一些信息丰富的图像区域，这些区域自然不同（"),e("RouterLink",{attrs:{to:"/pages/cd64f7/"}},[n._v("自底而上的显著性")]),n._v("）或与某些对象类别相关，这些对象类别由认知现象，如知识、期望、奖励和特定任务（自上而下的显著性）[7]。与OD类似，"),e("font",{attrs:{color:"red"}},[n._v("自底而上的SOD面临着来自无约束对象类别中大量外观变化的挑战，而自上而下的SOD面临着如何有效地将所需视觉刺激（通常在语义级别）与视觉场景中的相应区域相关联的挑战")]),n._v("。通常，SOD算法根据获得的"),e("a",{attrs:{href:"https://baike.baidu.com/item/%E6%98%BE%E8%91%97%E5%9B%BE/22742326#:~:text=%E5%9C%A8%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E4%B8%AD%EF%BC%8C%20%E6%98%BE%E8%91%97%E6%80%A7%20%E6%98%AF%E4%B8%80%E7%A7%8D%E5%9B%BE%E5%83%8F%E5%88%86%E5%8C%BA%E7%9A%84%E6%A8%A1%E5%BC%8F%EF%BC%8C%E8%80%8C%20%E6%98%BE%E8%91%97%E5%9B%BE%20%EF%BC%88%E8%8B%B1%E8%AF%AD%EF%BC%9A%20Saliency%20map,%EF%BC%89%E6%98%AF%E6%98%BE%E7%A4%BA%E6%AF%8F%E4%B8%AA%20%E5%83%8F%E7%B4%A0%20%E7%8B%AC%E7%89%B9%E6%80%A7%E7%9A%84%E5%9B%BE%E5%83%8F%E3%80%82%20%E6%98%BE%E8%91%97%E5%9B%BE%E7%9A%84%E7%9B%AE%E6%A0%87%E5%9C%A8%E4%BA%8E%E5%B0%86%E4%B8%80%E8%88%AC%E5%9B%BE%E5%83%8F%E7%9A%84%E8%A1%A8%E7%A4%BA%E7%AE%80%E5%8C%96%E6%88%96%E6%98%AF%E6%94%B9%E5%8F%98%E4%B8%BA%E6%9B%B4%E5%AE%B9%E6%98%93%E5%88%86%E6%9E%90%E7%9A%84%E6%A0%B7%E5%BC%8F%E3%80%82%20%E4%B8%BE%E4%BE%8B%E6%9D%A5%E8%AF%B4%EF%BC%8C%E6%9F%90%E4%B8%AA%E5%83%8F%E7%B4%A0%E5%9C%A8%E4%B8%80%E5%BC%A0%E5%BD%A9%E8%89%B2%E5%9B%BE%E4%B8%AD%E5%85%B7%E6%9C%89%E8%BE%83%E9%AB%98%E7%9A%84%20%E7%81%B0%E9%98%B6%20%EF%BC%8C%E5%85%B6%E4%BC%9A%E5%9C%A8%E6%98%BE%E8%91%97%E5%9B%BE%E4%B8%AD%E4%BB%A5%E8%BE%83%E4%B8%BA%E6%98%8E%E6%98%BE%E7%9A%84%E6%96%B9%E5%BC%8F%E8%A2%AB%E6%98%BE%E7%A4%BA%E5%87%BA%E6%9D%A5%E3%80%82",target:"_blank",rel:"noopener noreferrer"}},[n._v("显著图"),e("OutboundLink")],1),n._v("输出有限数量的对象区域，如图1（b）所示。它们还可以帮助执行各种计算机视觉任务，如"),e("font",{attrs:{color:"red"}},[n._v("图像检索")]),n._v("[8]和"),e("font",{attrs:{color:"red"}},[n._v("对象分割")]),n._v("[9]。")],1),n._v(" "),e("p",[n._v("​\t\t目标检测的第三个方向是"),e("font",{attrs:{color:"red"}},[n._v("COD")]),n._v("[10] [11]。与OD不同，COD的目标是从每个给定的图像中检测多个"),e("font",{attrs:{color:"red"}},[n._v("预定义的对象类别")]),n._v("。它不仅需要识别可能包含感兴趣对象的图像区域，还需要识别每个检测到的图像区域的特定对象类别。与SOD相比，COD具有完全不同的动机，即它移动在不了解人类视觉系统功能(例如视觉注意)的情况下"),e("font",{attrs:{color:"red"}},[n._v("解决纯计算问题")]),n._v("。通常，COD被转化为一个"),e("font",{attrs:{color:"red"}},[n._v("多分类")]),n._v("问题，在该问题中，识别性分类函数被训练来分离相应特征域中提取的图像区域。COD的主要挑战是如何处理组内外观变化和组间外观相似性。如图1（c）所示，COD方法通常会输出多个"),e("font",{attrs:{color:"red"}},[n._v("指定了识别对象类别的图像区域")]),n._v("。COD可以应用于"),e("font",{attrs:{color:"red"}},[n._v("场景解析")]),n._v("[12]和"),e("font",{attrs:{color:"red"}},[n._v("人类行为识别")]),n._v("[13]等计算机视觉任务。")],1),n._v(" "),e("p",[n._v("​\t\t为了解决目标检测中的挑战性问题，为了设计更好的手工特征（如"),e("font",{attrs:{color:"red"}},[n._v("HOG和SIFT")]),n._v("），已经提出了大量工作，并提出了复杂的目标检测框架，以便在整个目标检测开发阶段将提取的特征与精心设计的分类器（如"),e("font",{attrs:{color:"red"}},[n._v("随机森林和AdaBoost")]),n._v("）结合起来。卷积神经网络（CNN）于"),e("font",{attrs:{color:"red"}},[n._v("2004年")]),n._v("首次应用于目标检测[14]，自2013年以来得到了广泛应用[15]。[10]中关于基于区域的CNN（RCNN）的工作在2014年取得了重大突破。它最早致力于通过使用多层卷积网络来提取高分辨但不变的特征表示来描述目标检测系统。")],1),n._v(" "),e("p",[n._v("​\t\t与当时的最佳方法相比，这项工作的平均精度（mAP）显著提高了50%以上，这些方法基于常用PASCAL检测基准[16]上手工制作的图像特征。从那时起，已经提出了几种基于高级深度学习的技术[17]-[20]，用于高质量的目标检测，它涵盖了OD、SOD和COD的所有相关领域。为此，本文对最近最先进的方法进行了全面的综述。")]),n._v(" "),e("p",[n._v("​\t\t本文主要有四个动机：")]),n._v(" "),e("p",[n._v("1） 目标检测，包括OD、SOD和COD，是计算机视觉的一个基本但具有挑战性的问题。现有的调查论文只关注每个单独的主题，而没有讨论密切的关系。")]),n._v(" "),e("p",[n._v("2） 由于近年来已经提出了许多方法，并且取得了突破性的性能，因此回顾最近提出的目标检测技术，尤其是基于深度学习技术的目标检测技术，将是一件有启发性的事情。")]),n._v(" "),e("p",[n._v("3） 就几个重要问题进行深入讨论是非常有意义的。例如，为什么最近基于深度学习的框架能够显著提高目标检测的性能？与以前的框架相比，这种框架最本质的改进是什么？基于深度学习的方法在未来需要解决哪些问题？")]),n._v(" "),e("p",[n._v("4） 对公开的目标检测基准测试的实验结果进行综合比较和分析，将有助于读者更好地了解每种目标检测策略的性能以及相应的网络体系结构。")]),n._v(" "),e("h2",{attrs:{id:"预备知识"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#预备知识"}},[n._v("#")]),n._v(" 预备知识")]),n._v(" "),e("p",[n._v("​\t\t近年来，深度学习的研究领域得到了快速发展，包括其在计算机视觉中的普及。在本节中，我们将简要介绍在目标检测任务中广泛使用的一种高级深度学习技术，即CNN。")]),n._v(" "),e("p",[n._v("​\t\tCNN是受生物自然视觉感知机制启发的最著名、应用最广泛的深度学习架构之一，该机制最初由Fukushima[21]提出，后来由LeCun[22]改进。CNN设计用于处理以多个阵列形式出现的数据[23]，例如，由三个二维阵列组成的彩色图像，其中包含三个颜色通道中的像素强度。CNN利用了自然信号的特性，其背后有四个关键思想：本地连接、共享权重、池化和多层的使用[23]。")]),n._v(" "),e("p",[n._v("​\t\t如图2所示，典型CNN模型的体系结构由一系列层构成，如下所示：")]),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209162320080.png",alt:"image-20220209162320080"}})]),n._v(" "),e("center",[n._v("图2.典型CNN模型的架构。")]),n._v(" "),e("ul",[e("li",[e("p",[n._v("卷积层：卷积层是特征提取最重要的部分。前几层通常捕获低级特征（如边、线和角），而较深层则可以通过组合低级特征来学习高级特征（如结构、对象和形状）。卷积层中的每个单元通过一组称为滤波器组的内核连接到前一层特征图中的局部面片。然后，该局部加权和的结果通过非线性运算，例如校正线性单元（ReLU）。要素图中的所有单元共享同一个过滤器组。卷积层中的不同特征映射使用不同的滤波器组。")])]),n._v(" "),e("li",[e("p",[n._v("池化层：池化层旨在降低表示的维度，并创建对小位移和扭曲的不变性。池化层通常位于两个卷积层之间。池层的每个特征映射都连接到前一个卷积层的对应特征映射。一个典型的池化单元计算一个特征图中单元的一小块局部的最大值。")])]),n._v(" "),e("li",[e("p",[n._v("全连接层：全连接层通常用作网络的最后几层，以便更好地总结低层在最终决策中传达的信息。由于完全连接的层占据了大部分参数，因此很容易发生过拟合。为了防止这种情况，通常采用dropout[24]。从2012年Alex Net[24]在ImageNet分类方面取得突破性成功开始，在开发各种CNN模型方面做出了重大努力，包括VGGNet[25]、GoogLeNet[26]和ResNet[27]。")])]),n._v(" "),e("li",[e("p",[n._v("AlexNet:AlexNet[24]最早由Krizhevsky等人提出，并赢得了2012年ImageNet大规模视觉识别挑战赛（ILSVRC）[28]。它由五个卷积层和三个完全连接的层组成。这是计算机视觉和机器学习的一个里程碑式的研究，因为它是第一个采用非饱和神经元、图形处理单元（GPU）实现卷积运算和dropout以防止过度拟合的工作。")])]),n._v(" "),e("li",[e("p",[n._v("VGGNet:VGGNet[25]是ILSVRC 2014大赛本地化和分类赛道的获胜者。它有两个著名的体系结构：VGGNet-16和VGGNet-19。前者因其更简单的架构而被广泛使用，它有13个卷积层、5个池化层和3个完全连接的层。")])]),n._v(" "),e("li",[e("p",[n._v("GoogLeNet：GoogLeNet[26]是另一个具有代表性的CNN架构，它有两个主要优势。一个是在同一层使用不同大小的过滤核，这保留了更多的空间信息，另一个优点是减少了网络的参数数量，这使得网络对过度拟合的敏感性降低，并允许网络更深入。事实上，22层的GoogLeNet有50多个卷积层分布在初始模块内部，但其参数比AlexNet少12倍。")])]),n._v(" "),e("li",[e("p",[n._v("ResNet:ResNet[27]是最成功的CNN之一，并获得了2016年计算机视觉和模式识别大会最佳论文奖。ResNet背后的思想是，每一层不应该学习整个特征空间变换，而应该只学习对前一层的剩余校正，这样可以有效地训练更深层次的网络。其极深的表示具有优异的泛化性能，并使其在2015ILSVRC和COCO竞赛中获得ImageNet检测、ImageNet定位、COCO检测和COCO分割的第一名。")])])]),n._v(" "),e("h2",{attrs:{id:"目标检测的现代方法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#目标检测的现代方法"}},[n._v("#")]),n._v(" 目标检测的现代方法")]),n._v(" "),e("h3",{attrs:{id:"od中的现代方法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#od中的现代方法"}},[n._v("#")]),n._v(" OD中的现代方法")]),n._v(" "),e("p",[n._v("OD的目标是选择一小组建议的对象，覆盖了给定图像中大多数感兴趣的对象。为了实现这一目标，OD方法需要1）生成或选择可能包含特定感兴趣对象的"),e("font",{attrs:{color:"red"}},[n._v("潜在边界框")]),n._v("，2）推断所选边界框的对象性"),e("font",{attrs:{color:"red"}},[n._v("分数")]),n._v("。我们通常可以将现有的OD方法分为三大类："),e("font",{attrs:{color:"red"}},[n._v("区域合并、窗口选择和框回归")]),n._v("。")],1),n._v(" "),e("h4",{attrs:{id:"区域合并方法-region-merging"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#区域合并方法-region-merging"}},[n._v("#")]),n._v(" 区域合并方法（Region-merging）")]),n._v(" "),e("p",[n._v("区域合并方法试图通过合并多个局部图像区域（例如超像素）来生成建议的对象。一种具有代表性的区域合并方法是著名的"),e("font",{attrs:{color:"red"}},[n._v("选择性搜索方法")]),n._v("[29]，该方法采用贪婪算法将图像区域迭代地分组在一起。具体来说，首先"),e("font",{attrs:{color:"red"}},[n._v("计算了所有相邻区域之间的相似性。之后，将两个最相似的区域组合在一起，并计算出结果区域与其相邻区域之间的新相似性。重复这样的分组过程，直到整个图像成为一个区域")]),n._v("。沿着这一方向，最近提出了更多的方法来更好地解决这个问题。例如，Krahenbuhl等人[30]开发了一种基于学习的种子放置方法，用于识别一组种子超像素，以命中每个给定图像中的所有对象。然后，他们合并靠近每个种子超像素的图像区域，并计算有符号测地距离变换(signed geodesic distance transform)，以提取一小部分高质量的建议对象。Bazzani等人[31]早期致力于采用深度学习技术，并提出了一种新的基于区域合并的OD方法。具体地说，他们首先生成矩形区域的初始集合，即包围提取片段的边界框。然后，他们通过最大化包含四项的相似性函数贪婪地合并这两个区域。这四项是通过在ImageNet上预训练的CNN计算的[28]，分别包括分类分数下降的相似性、深层特征的相似性、图像的覆盖面积和空间位置的距离。")],1),n._v(" "),e("h4",{attrs:{id:"窗口选择方法-window-selecting"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#窗口选择方法-window-selecting"}},[n._v("#")]),n._v(" 窗口选择方法（Window-selecting）")]),n._v(" "),e("p",[n._v("窗口选择方法试图通过"),e("font",{attrs:{color:"red"}},[n._v("评分")]),n._v("和"),e("font",{attrs:{color:"red"}},[n._v("选择预生成（滑动）窗口")]),n._v("来生成建议对象。[1]提出了一种最早也是最著名的基于窗口选择的OD方法。它首先从给定图像中的显著位置[32]获得了一组初始方案。然后，这些建议通过组合多个信息线索进行评分，包括颜色对比度、边缘密度、位置、大小和“超像素跨越”(superpixel straddling)线索。受这项工作的启发，研究人员在过去几年进行了广泛的研究。例如，Ghodrati等人[17]最早利用经过预训练的CNN的深度特征图。其主要思想是在不同的激活层上以滑动窗口的方式生成假设。提出的逆级联搜索(inverse cascade searching)从CNN的最终卷积层到初始卷积层，可以选择最有希望的对象位置，并以从粗到细的方式细化它们的框。类似地，Pinheiro等人[33]，[34]提出使用带有两个分支的CNN来同时推断每个输入图像窗口的分割掩码和对象性分数。最终的包围盒方案是通过获取包围分割模板的矩形图像区域获得的。与[17]和[33]不同，Kuo等人[2]采用数据驱动的语义方法对对象提案进行排序。他们学习了一种小型但有效的CNN架构，用于重新排列自底而上方法得到的proposals[35]。在[36]中，Hu等人从卷积特征图中提取密集滑动窗口，然后采用统一的头部模块对窗口特征进行解码，并生成输出置信度得分和目标掩码。")],1),n._v(" "),e("h4",{attrs:{id:"盒回归法-box-regressing"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#盒回归法-box-regressing"}},[n._v("#")]),n._v(" 盒回归法（Box-regressing）")]),n._v(" "),e("p",[n._v("盒回归方法试图通过"),e("font",{attrs:{color:"red"}},[n._v("直接学习回归函数，从提取的深度特征图中获得边界盒位置和对象性分数，从而生成对象建议")]),n._v("。随着计算机视觉中深度学习技术的成功，这些方法应运而生。具体而言，Erhan等人[37]于2014年提出了第一种基于盒回归的OD方法，其中OD被定义为对边界盒位置坐标的回归问题。此外，它还推断出每个边界框的置信度得分，这表明该框包含对象的可能性有多大。整个系统由一个单一的CNN模型实现，该模型具有一个新的损失函数，该函数考虑了位置和得分精度。Szegedy等人[38]在[37]的基础上，基于最新的Inception-style架构[39]进一步构建了框架，并利用了包围盒形状和置信度的多尺度卷积预测，从而建立了基于Inception的后分类(post-classification)模型。沿着这个方向，Ren等人[40]提出了一个区域建议网络（RPN）来学习回归函数，用于基于一组预定义的平移不变锚(predefined translation-invariant anchors)来拟合边界框位置的坐标。他们还将RPN与一些额外的网络层结合起来，以实现精确的COD。类似地，Kong等人[41]开发了一种新的超特征(hyperfeature)，将深度但粗糙的信息与浅层但精细的信息相结合，以提取更丰富的特征。Li等人[42]建立了用于生成建议对象的缩小和放大网络(zoom-out-and-in network)，其中，放大子网络(zoom-in subnetwork)用于通过反卷积(deconvolution)操作提高高级特征的分辨率，递归训练管道用于在训练阶段连续回归区域proposals。")],1),n._v(" "),e("h3",{attrs:{id:"sod的现代方法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sod的现代方法"}},[n._v("#")]),n._v(" SOD的现代方法")]),n._v(" "),e("p",[n._v("SOD包括两个分支：自下而上和自上而下。前者是刺激驱动(stimulus driven)的，主要对视觉场景中"),e("font",{attrs:{color:"red"}},[n._v("最有趣和最显眼的区域")]),n._v("做出反应，而后者则由知"),e("font",{attrs:{color:"red"}},[n._v("识和高级视觉任务来指导")]),n._v("。例如"),e("font",{attrs:{color:"red"}},[n._v("故意寻找特定类别的对象")]),n._v("。如之前的研究[7]、[43]和[44]所述，在自底向上的SOD分支中，方法是检测自由观看下的显著性，这是由场景的物理特征自动确定的，而另一分支中的方法是检测由观察者当前目标确定的任务驱动的显著性。在每个分支中，都可以建立有监督和无监督的框架来解决相应的问题。接下来，我们将更详细地研究这两个分支。")],1),n._v(" "),e("h4",{attrs:{id:"自底向上的sod-bottom-up"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#自底向上的sod-bottom-up"}},[n._v("#")]),n._v(" 自底向上的SOD(Bottom-up)")]),n._v(" "),e("p",[n._v("自底向上的SOD旨在"),e("font",{attrs:{color:"red"}},[n._v("准确区分视觉场景中的前景对象和背景")]),n._v("。传统模型主要依赖"),e("font",{attrs:{color:"red"}},[n._v("对比度提示")]),n._v("。Cheng等人[6]提出的一种具有代表性的方法测量了归一化颜色直方图中每个图像区域和图像中所有其他区域之间的色差加权和，作为检测显著性的全局对比度。受这项工作的启发，一些研究人员还将局部和全局对比度结合起来进行显著性检测。近年来，随着深度学习的发展，深度神经网络（DNN）也被用来提高SOD的性能。最早的开创性工作之一是[45]，其中Han等人提出利用叠加去噪自动编码器对SOD之前的背景进行建模。除了这项工作，最近几年还提出了一些基于CNN的SOD方法。例如，Wang等人[46]提出将局部估计和全局搜索结合起来进行显著性检测。Lee等人[47]结合了每个超像素的低层距离图和整个图像的全局CNN特征。Liu和Han[20]提出，以端到端的方式，从全局到局部上下文，从粗到细分层检测显著对象。Li和Yu[48]提出将基于像素级完全卷积网络（FCN）的显著性网络与分段多尺度CNN相结合，用于显著性检测。Wang等人[49]通过递归FCN提出了一种渐进的显著性细化网络，在该网络中，先前的显著性图和原始图像被同时馈送(simultaneously fed)，以学习纠正其先前的错误，从而获得更好的显著性结果。")],1),n._v(" "),e("h4",{attrs:{id:"自顶向下的sod-top-down"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#自顶向下的sod-top-down"}},[n._v("#")]),n._v(" 自顶向下的SOD(Top-down)")]),n._v(" "),e("p",[n._v("自上而下的SOD通常旨在"),e("font",{attrs:{color:"red"}},[n._v("高亮显示场景中特定于类别的对象")]),n._v("。Yang和Yang[50]提出联合学习条件随机场的参数和一个用于监督自上而下显著性检测的字典。He等人[51]提出了一种基于样本的自上而下显著性检测方法，目的是定位与给定样本图像属于同一类别的对象。Cholakkal等人[52]提出了一种只使用图像标签的弱监督自上而下显著性框架。他们首先使用图像标签训练了一个基于稀疏编码的空间金字塔匹配（ScSPM）分类器。然后分析图像中每每一小块对分类器的概率贡献，以估计反向SCSPM显著性(reverse-ScSPM saliency)。接下来，使用逻辑回归模型，使用每一小块的上下文(contextual patches)来估计上下文显著性。最终的显著性图可以通过组合两个显著性图来获得。Zhang等人[53]基于自顶向下的赢家通吃(winner-take-all)过程和DNN中的反向传播，提出了自顶向下显著性检测的激励反向传播方法。")],1),n._v(" "),e("h3",{attrs:{id:"cod的现代方法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#cod的现代方法"}},[n._v("#")]),n._v(" COD的现代方法")]),n._v(" "),e("p",[n._v("​\t\t在过去的几十年里，COD在文献中得到了广泛的研究。"),e("font",{attrs:{color:"red"}},[n._v("变形零件模型（DPM）")]),n._v("[54]及其变体多年来一直是主流方法。这些方法使用手工制作的图像描述符作为特征，并扫描整个图像，以检测具有特定于类的最大响应的区域。")],1),n._v(" "),e("p",[n._v("​\t\t最近，由于ImageNet[28]等大规模训练数据的可用性和高性能GPU的进步，人们提出了各种基于深度学习的方法（尤其是基于CNN的方法），以显著提高COD的技术水平。事实上，CNN用于检测和识别可以追溯到20世纪80年代[22]。然而，由于缺乏训练数据和有限的计算资源，在2012年之前，基于CNN的COD没有多少进展。自2012年CNN在ILSVRC的图像分类任务中取得突破性成功[28]以来，基于CNN的范例（用于COD）最近吸引了大量研究兴趣。COD方法通常有两大类："),e("font",{attrs:{color:"red"}},[n._v("基于建议对象的方法和基于回归")]),n._v("的方法。")],1),n._v(" "),e("h4",{attrs:{id:"基于建议目标的方法-object-proposal-based"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基于建议目标的方法-object-proposal-based"}},[n._v("#")]),n._v(" 基于建议目标的方法(Object proposal-based)")]),n._v(" "),e("p",[n._v("​\t\t基于建议对象的COD框架"),e("font",{attrs:{color:"red"}},[n._v("首先使用建议区域方法（如选择性搜索[29]）生成一组建议边界框，其中可能包含对象（该过程也称为OD），然后将检测到的对象建议传递给CNN分类器，以确定它们是背景还是来自特定区域对象类")]),n._v("。")],1),n._v(" "),e("p",[n._v("​\t\t在各种基于对象提议的方法（COD）中，Girshick等人[10]在2014年提出的区域"),e("font",{attrs:{color:"red"}},[n._v("CNN（R-CNN）")]),n._v("的工作是最著名的方法之一。这项工作为通过深度CNN模型提取丰富的特征打开了大门，从而显著提高了性能。R-CNN框架是一系列概念上简单的步骤：生成对象建议，将建议分类为背景或特定类别的对象，并对检测进行后处理，以提高它们对对象的适应性。简而言之，R-CNN的工作如下。首先，它通过选择性搜索算法[29]提取大约2000个自下而上的区域建议，其中可能包含对象，以降低计算成本。然后，将这些建议区域变形(warped)为固定大小（例如227 x 227），并使用微调的CNN模型从中提取CNN特征。接下来，使用特定类别的线性支持向量机（SVM）将每个建议区域分类为对象或非对象。最后，通过使用边界盒回归器[54]来改进定位，候选方案被重新调整为检测到的对象。这个简单的管道在基准数据集上实现了最先进的COD性能，与所有以前发表的作品（主要基于DPM）相比，性能有了显著提高[54]。在这里，值得一提的是，用于从建议区域中提取深度CNN特征的CNN模型通常在基于ImageNet数据集的图像分类辅助任务[28]上进行预训练，然后在检测任务的带边框注释的小图像集上进行微调。")],1),n._v(" "),e("p",[n._v("​\t\t然而，在R-CNN中，我们必须反复将候选边界框调整为固定大小，以提取其CNN特征，这对COD来说是非常昂贵的。为了加快R-CNN的速度，一些文献[18]、[55]、[56]建议在特征提取中共享计算(share the computation)。例如，空间金字塔池网络（SPPnet）[55]引入了空间金字塔池层，以放松输入必须具有固定大小的约束。与R-CNN不同，SPPnet只从整个图像中提取一次特征图，与建议区域无关，然后对每个区域建议应用空间金字塔池以获得固定长度的表示。这种重组允许所有区域提案之间轻松共享计算。SPPnet的一个缺点是，SPPnet的微调算法只能更新完全连接的层，这使得无法联合训练CNN特征提取器和SVM分类器来进一步提高性能。为了修正这个缺点，提出了fast R-CNN[18]，它是SPPnet的端到端可训练的改进。在fast R-CNN的框架下。所有网络层都可以在微调过程中更新，从而简化学习过程，提高检测精度。")]),n._v(" "),e("p",[n._v("​\t\tR-CNN[10]和fast R-CNN[18]的框架都需要建议区域的输入，这些提案通常来自手工制作的建议区域方法，如选择性搜索[29]和EdgeBox[35]。然而，proposal生成是整个流程中的瓶颈。为了解决这个问题，提出了faster R-CNN[40]，它由两个模块组成。第一个被称为区域建议网络（RPN），是用于生成建议区域的FCN（每以个都有一个建议边界框和一个目标分数），并将其输入第二个模块。第二个模块是用于目标检测的fast R-CNN网络。faster  R-CNN将建议生成和目标检测结合到一个统一的网络中，其中RPN模块与fast R-CNN检测网络共享相同的卷积特征；因此，它几乎可以无代价生成建议区域。")]),n._v(" "),e("h4",{attrs:{id:"基于回归的方法-regression-based"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基于回归的方法-regression-based"}},[n._v("#")]),n._v(" 基于回归的方法(Regression-based)")]),n._v(" "),e("p",[n._v("​\t\t基于回归的COD方法被描述为具有空间分离的边界盒和相关类别概率的回归问题[57]-[60]。与基于建议对象的方法相比，基于回归的框架（用于COD）要简单得多，因为它不需要建议生成和随后的像素/特征重采样阶段，并将所有阶段封装在单个网络中[58]。请注意，box回归OD法和回归COD法之间的主要区别在于前者的目标是预测box位置和每个box位置的一个客观评分，后者的目标是预测box的位置和每个box位置的目标类别分数（其维度取决于所需目标类别的数量）。本质上，回归COD设计的模型通常比box回归OD设计的模型复杂得多，因为前者需要同时处理建议定位和对象类别识别的任务。因此，多任务损失函数在基于回归的COD中比在box回归OD中更常用。"),e("font",{attrs:{color:"red"}},[n._v("You Only Look Once (YOLO)")]),n._v("[57]和"),e("font",{attrs:{color:"red"}},[n._v("Single-Shot MultiBox（SSD）")]),n._v("[58]是两种典型的基于回归的方法（用于COD）。")],1),n._v(" "),e("p",[n._v("​\t\tYOLO[57]将基于CNN的目标检测描述为一个回归问题，从而开启了实现实时目标检测的大门。YOLO的独特之处在于，它将目标检测的各个部分统一到一个卷积网络中，该网络可以同时预测多个边界框和这些边界框的种类概率。神经网络在进行预测时会对图像进行全局推理，因此它会隐式地对种类及其外观的上下文信息进行编码。与基于建议目标的方法相比，YOLO速度非常快，每秒运行45-150帧，而无需在Titan X GPU上进行批处理。然而，仍然"),e("font",{attrs:{color:"red"}},[n._v("很难检测到小尺寸")]),n._v("的物体并实现精确定位。")],1),n._v(" "),e("p",[n._v("​\t\t此后，SSD[58]被提出用于改进YOLO方法。具体来说，它将边界框的输出空间离散为一组默认框，每个特征图位置具有不同的纵横比和比例，与faster R-CNN的RPN有类似的想法。在预测时，SSD导出每个默认框中存在的每个对象类别的分数，并生成对框的调整，以更好地匹配对象外观。此外，该网络将来自不同分辨率的多个特征图的预测结合起来，以处理不同大小的对象。通过引入多尺度特征图和默认框机制，SSD在检测小尺寸对象方面取得了显著的性能改进，与YOLO相比，还提高了定位精度。")]),n._v(" "),e("p",[n._v("​\t\t此外，最近还开展了一些工作，以进一步提高基于CNN的COD方法的性能，如硬负挖掘(hard negative mining)[61]、特征增强[41]、[62]，上下文信息融合[63]-[65]，等等。例如，为了提高处理具有挑战性的情况的能力，通过对象旋转、类内可变性和类间相似性，Cheng等人[62]提出了一种旋转不变的Fisher判别CNN模型。在现有高容量CNN架构的基础上，通过分别引入旋转不变层和Fisher判别层来实现。")]),n._v(" "),e("h2",{attrs:{id:"od、sod、cod之间的关系"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#od、sod、cod之间的关系"}},[n._v("#")]),n._v(" OD、SOD、COD之间的关系")]),n._v(" "),e("p",[n._v("虽然OD、SOD和COD是目标检测中的三个独立研究方向，但它们之间存在着丰富的关系。")]),n._v(" "),e("h3",{attrs:{id:"od与sod的关系"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#od与sod的关系"}},[n._v("#")]),n._v(" OD与SOD的关系")]),n._v(" "),e("p",[n._v("​\t\t一方面，自下而上的SOD能够为OD提供信息丰富的先验知识。直观地说，对人类视觉系统更具吸引力（在图像场景中更突出）的提取边界框位置更有可能包含感兴趣的对象。基于这一观察，已经设计了几种基于显著性线索的OD方法。例如，最经典的OD方法之一[1]通过使用三个显著性提示来选择对象方案，即多尺度显著性、颜色对比度和边缘密度。类似地，[66]中的工作将目标性定义为窗口显著性，这是使用图像的剩余部分构成窗口的成本。这个定义基本上包含了全局稀有性原则(global rarity principle)，并将其从像素级（对于SOD）扩展到窗口级（对于OD）。此外，Cheng等人将OD视为自底向上SOD的特例[67]，这表明利用SOD的检测原理可以有效地制定OD。Erhan等人[37]还提出了一种基于显著性的OD神经网络模型，并取得了良好的性能。")]),n._v(" "),e("p",[n._v("​\t\t另一方面，一些自底而上的SOD方法也建立在OD结果的基础上。当有OD生成的边界盒时，SOD问题可以简化为从非显著边界盒中选择显著边界盒。基于这一直觉，Chang等人[68]提出通过一个统一的图形模型，在检测显著对象之前，集成对象性先验（包括对象大小和位置）和显著性。Jiang等人[69]将客观性优先与聚焦性和客观性相结合，以保持检测到的SOD显著区域的完整性。Li等人[70]提出将从注视预测中获得的具有高显著值的候选对象视为显著对象。与传统的显著性检测只需要突出显示不同的局部区域不同，SOD需要均匀地突出显示完整的显著性对象，因此对象的感知能力应该自然地编码到有效的SOD模型中。目标性优先自然为这一需求提供了有效的解决方案。")]),n._v(" "),e("h3",{attrs:{id:"sod与cod的关系"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sod与cod的关系"}},[n._v("#")]),n._v(" SOD与COD的关系")]),n._v(" "),e("p",[n._v("​\t\t由于自上而下的SOD是高度任务驱动和知识驱动的，它需要对视觉场景，尤其是场景中对象的类别级信息有较高的理解。为了实现在场景中定位目标的目标，自上而下的SOD方法通常需要获取自上而下的知识来指导检测过程[51]。这种自上而下的知识可能来自记忆（即，使用来自相应训练数据的知识定位场景中的对象，这是基于模型的对象检测）或对象关联（即，使用已知或未知的样本定位场景中的相应对象，这是基于样本的对象检测[71]-[73]）。例如，在[50]中，Yang等人通过联合CRF和字典学习检测自上而下的显著性。CRF模型是通过在图像贴片表示(image patch representation)和相应的贴片标签(patch labels)上训练线性支持向量机来初始化的，它本质上是一个贴片级别(patch-level)的类别特定对象检测器。在以前的工作中，也可以找到一些实验结果或讨论，表明某些特定类别的物体检测器（例如，人类、人脸、汽车、出现在给定图像中的单词等）提供的自上而下的线索在视觉注意机制中起着重要作用[74]，[75]")]),n._v(" "),e("p",[n._v("​\t\t除了SOD，自上而下的SOD尤其可以反过来为COD提供有用的特定类别的对象，尤其是在监管薄弱的情况下。正如我们所知，弱监督对象检测方法[3]、[76]、[77]旨在仅使用图像级标记而不是实例级边界框注释来学习特定于类别的对象检测器。在这种情况下，如何获得特定对象类别的初始对象位置是需要解决的主要问题。通过使用SOD方法，[3]和[76]有效地初始化特定类别的对象位置，然后采用迭代学习方案，以迭代方式逐步细化对象检测器和位置。当学习过程收敛时，可以学习更强的目标检测器来执行测试数据中的COD。参考文献[52]、[53]和[77]还提出应用自上而下的显著性检测来发现弱标记训练图像中的目标位置，随后可用于训练特定类别的目标检测器。")]),n._v(" "),e("h3",{attrs:{id:"cod与od的关系"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#cod与od的关系"}},[n._v("#")]),n._v(" COD与OD的关系")]),n._v(" "),e("p",[n._v("​\t\t大量研究表明，OD可以直接受益于COD任务[10]、[11]、[18]、[55]。基本上，正如我们在“基于建议对象的方法”一节中总结的那样，COD方法的一个主流是建立在OD技术上的，其中OD可以作为单独的预处理步骤[10]或在统一对象检测框架[40]中设计的固有组件来工作。COD方法基于OD技术的构建通常比基于滑动窗口搜索策略的构建获得更好的性能[54]，因为OD技术可以在目标检测任务之前提供有用的位置，这可以极大地减少对许多背景图像区域不必要的搜索，从而有效地减少误报。")]),n._v(" "),e("p",[n._v("​\t\t大多数OD方法中的参数需要从收集的训练集中学习，这些训练集通常来自PASCAL VOC基准[78]。本质上，由于训练数据集中只有有限的对象类，因此此类训练数据（包含不到20个类别的对象的真值边界框）可以被视为学习对象性检测器的受限知识库（例如[19]、[40]和[58]）。尽管一些工作已经证明了他们提出的方法，[2]和[17]仍然能够为看不见的目标生成建议目标，即不包含在训练数据集中的目标类；由于大量的领域转移，这些方法可能或多或少受到性能下降的影响。值得一提的是，最近的一项研究[31]提出了特定类别的OD方法，其目标与COD方法类似。")]),n._v(" "),e("h2",{attrs:{id:"基准和评价指标"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基准和评价指标"}},[n._v("#")]),n._v(" 基准和评价指标")]),n._v(" "),e("h3",{attrs:{id:"od的基准"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#od的基准"}},[n._v("#")]),n._v(" OD的基准")]),n._v(" "),e("p",[n._v("​\t\tOD中广泛使用了两个基准："),e("font",{attrs:{color:"red"}},[n._v("PASCAL VOC 2007")]),n._v("[79]的测试集和"),e("font",{attrs:{color:"red"}},[n._v("MS COCO")]),n._v("[80]的验证集。具体来说，PASCAL VOC 2007的测试集包含来自20个类别的4952个图像和14976个对象实例。大量的"),e("font",{attrs:{color:"red"}},[n._v("对象和种类、视点、比例、位置、遮挡和照明的高度多样性")]),n._v("使得该数据集非常流行于评估OD方法，因为OD的目标是在不同的图像场景中找到所有可能的对象。MS COCO基准包含80000名训练图像和总计约500000个实例注释。该数据集中的图像是从复杂的日常场景中收集的，这些场景包含自然环境中的常见对象。因此，它是一个更具挑战性的数据集，用于检测对象性建议。在大多数情况下，评估OD性能的实验是在前5000张MS COCO验证图像上进行的。有时，使用另一个非重叠(nonoverlapped)图像作为验证数据集。")],1),n._v(" "),e("h3",{attrs:{id:"sod的基准"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sod的基准"}},[n._v("#")]),n._v(" SOD的基准")]),n._v(" "),e("p",[n._v("​\t\tSOD社区中有几个具有不同属性的基准数据集。"),e("font",{attrs:{color:"red"}},[n._v("ECSSD数据集")]),n._v("[81]有1000个图像，在SOD数据集中有一些重叠的图像[82]。这两个数据集中的图像通常具有杂乱的背景和语义上有意义的前景对象，这些对象来自不同的位置和比例。"),e("font",{attrs:{color:"red"}},[n._v("PASCAL-S")]),n._v("[70]数据集是基于PASCAL VOC分割挑战而建立的，它有850张图像，通常包含杂乱的背景和多个前景对象。"),e("font",{attrs:{color:"red"}},[n._v("HKU-IS")]),n._v("[83]数据集是最近发布的SOD数据集，包含4447幅图像。这些图像是从许多具有挑战性的场景中收集的，这些场景中有多个不连续的突出物体，突出物体接触图像边界，颜色对比度低。"),e("font",{attrs:{color:"red"}},[n._v("DUT-OMRON数据集")]),n._v("[84]由5168个图像组成，每个图像通常有复杂的背景，包含一个或两个前景对象。"),e("font",{attrs:{color:"red"}},[n._v("THUR15K")]),n._v("数据集[85]包含来自五个对象类的6232幅图像，即蝴蝶、咖啡杯、狗跳、长颈鹿和飞机。此数据集中的一些图像没有前景对象。"),e("font",{attrs:{color:"red"}},[n._v("MSRA-10K数据集")]),n._v("[6]包含10000张带有各种对象的图像，是"),e("font",{attrs:{color:"red"}},[n._v("MSRA-B数据集")]),n._v("[5]的扩展。这两个数据集中的大多数图像只有一个前景对象和清晰的背景。"),e("font",{attrs:{color:"red"}},[n._v("SED数据集")]),n._v("是另一个广泛使用的数据集，包含200幅图像。此数据集中的每个图像包含一个和两个前景对象。")],1),n._v(" "),e("h3",{attrs:{id:"cod的基准"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#cod的基准"}},[n._v("#")]),n._v(" COD的基准")]),n._v(" "),e("p",[n._v("​\t\t"),e("font",{attrs:{color:"red"}},[n._v("PASCAL VOC 2007")]),n._v("[79]和"),e("font",{attrs:{color:"red"}},[n._v("PASCAL VOC 2012")]),n._v("[16]数据集是评估各种目标检测方法最常用的两个基准。PASCAL VOC 2007数据集共包含来自20个对象类别的9963张图像，包括5011张用于训练和验证的图像和4952张用于测试的图像，其中20个对象类别的地面真值边界框被手动标记。PASCAL VOC 2012数据集是PASCAL VOC 2007数据集的扩展，该数据集共包含22531幅图像，包括用于训练和验证的11540幅图像和用于测试的10991幅图像。然而，测试集中没有提供基本的真值标签。因此，应通过将测试结果提交给PASCAL VOC评估服务器来评估所有方法。")],1),n._v(" "),e("p",[n._v("​\t\t"),e("font",{attrs:{color:"red"}},[n._v("MS COCO")]),n._v("[80]是2014年提出的一个较新的目标检测基准，其目标是通过将目标识别问题置于更广泛的场景理解问题的背景下，提升目标识别的最新水平。与PASCAL VOC数据集相比，该数据集在每个类别的实例数上要大得多。具体来说，该数据集包含200000多个图像和80个对象类别，其中训练集包含80000个图像，验证集包含40000个图像，测试集包含80000个图像。为了限制过度拟合，并让研究人员更灵活地测试他们的方法，测试集分为三部分，包括测试开发、测试标准和测试挑战。Test dev用于调试和验证实验，并允许无限提交到评估服务器。测试标准用于维护提交后更新的公共排行榜。测试挑战用于比赛。大多数已发表的作品都在测试开发集上报告了它们的检测结果。")],1),n._v(" "),e("h3",{attrs:{id:"od的评价指标"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#od的评价指标"}},[n._v("#")]),n._v(" OD的评价指标")]),n._v(" "),e("p",[n._v("​\t\tOD方法生成的评估建议对象的指标通常是建议位置和相应真值注释之间的联合交集（IOU）（或Jaccard索引）的函数。具体而言，特定提案位置"),e("mjx-container",{staticClass:"MathJax",staticStyle:{direction:"ltr"},attrs:{jax:"SVG"}},[e("svg",{staticStyle:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.439ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"1.878ex",height:"1.439ex",role:"img",focusable:"false",viewBox:"0 -442 830 636"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"msub"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D45D",d:"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"}})]),e("g",{attrs:{"data-mml-node":"mi",transform:"translate(536,-150) scale(0.707)"}},[e("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D456",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"}})])])])])])]),n._v("和地面真相注释"),e("mjx-container",{staticClass:"MathJax",staticStyle:{direction:"ltr"},attrs:{jax:"SVG"}},[e("svg",{staticStyle:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.464ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"1.819ex",height:"1.464ex",role:"img",focusable:"false",viewBox:"0 -442 804 647"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"msub"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D454",d:"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"}})]),e("g",{attrs:{"data-mml-node":"mi",transform:"translate(510,-150) scale(0.707)"}},[e("path",{staticStyle:{"stroke-width":"3"},attrs:{"data-c":"1D456",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"}})])])])])])]),n._v("之间的IOU定义为：")],1),n._v(" "),e("center",[e("p",[e("img",{attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209215517848.png",alt:"image-20220209215517848",loading:"lazy"}})])]),n._v(" "),e("p",[n._v("​\t\t基于IOU，召回率可以计算为建议位置覆盖的ground-truth边界框在某个IOU重叠阈值以上的分数。然后，三个广泛用于评估对象性检测方法的评估指标如下：")]),n._v(" "),e("ul",[e("li",[e("font",{attrs:{color:"red"}},[n._v("召回与建议曲线")]),n._v("，描述不同建议数量的召回。")],1),n._v(" "),e("li",[e("font",{attrs:{color:"red"}},[n._v("召回与重叠曲线")]),n._v("，说明了在不同IOU重叠标准下召回的变化。")],1),n._v(" "),e("li",[e("font",{attrs:{color:"red"}},[n._v("平均召回率（AR）")]),n._v("，计算“召回与重叠”曲线下重叠值范围内的面积（通常设置为0.5-1.0）。")],1)]),n._v(" "),e("h3",{attrs:{id:"sod的评价指标"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sod的评价指标"}},[n._v("#")]),n._v(" SOD的评价指标")]),n._v(" "),e("p",[n._v("​\t\t通常三个标准的评估指标被广泛用于SOD。第一个是"),e("font",{attrs:{color:"red"}},[n._v("精确召回曲线（PRC）")]),n._v("。具体来说，给定一个显著性映射S和相应的真值显著性掩码G，我们首先将S标准化为[0]，[1]。然后，我们使用阈值T将S转换为二进制掩码M。之后，可以在阈值T下计算精度和召回率。当T从0变为1时，我们可以获得一系列精度召回值对。因此，我们可以通过将SOD作为分类任务来绘制PRC来评估模型的性能。")],1),n._v(" "),e("p",[n._v("​\t\t第二个指标是F测量分数，它综合考虑了精确度和召回率。通常情况下，首先使用自适应阈值分割显著性图，并获得精度和召回值；然后，将F度量分数计算为它们的加权调和平均值。")]),n._v(" "),e("p",[n._v("​\t\t虽然这两种度量被广泛使用，但它们不能考虑真实的负像素(negative pixels)。为了解决这个问题，也常用"),e("font",{attrs:{color:"red"}},[n._v("平均绝对误差（MAE）")]),n._v("。MAE测量S和G之间的平均像素绝对差。有关上述评估指标的更多详细信息，请参考[20]、[83]和[87]。")],1),n._v(" "),e("h3",{attrs:{id:"cod的评价指标"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#cod的评价指标"}},[n._v("#")]),n._v(" COD的评价指标")]),n._v(" "),e("p",[n._v("​\t\t所有对象类的"),e("font",{attrs:{color:"red"}},[n._v("平均精度（AP）和mAP")]),n._v("是评估各种对象检测方法的两个标准且广泛使用的指标。它们被设计成不太喜欢缺少对象实例、重复检测一个实例和假阳性检测(false positive detections)的方法。具体来说，AP计算不同召回级别的平均准确度值，即PRC下的区域，因此AP值越高，性能越好，反之亦然。精确性衡量的是检测到的真正阳性的分数，而召回率衡量的是正确检测到的阳性的分数。")],1),n._v(" "),e("p",[n._v("​\t\t如果预测边界框和真值边界框之间的IOU超过预定义阈值，则方法的检测输出被指定为真阳性(true positive)。否则，检测被视为假阳性(false positive)。此外，如果多个检测输出与同一地面真值对象重叠，则只有一个被视为真阳性，其他被视为假阳性。")]),n._v(" "),e("p",[n._v("​\t\t对于PASCAL VOC数据集，面积重叠阈值通常设置为0.5[16]。对于COCO数据集，存在一个新的评估指标，即在不同的IOU阈值上平均mAP，从0.5到0.95，步长为0.05（写为0.5:0.95）。与PASCAL VOC指标相比，它更强调本地化，后者只需要0.5的IOU。有关上述评估指标的更多详细信息，请参阅[40]、[58]、[60]。")]),n._v(" "),e("h2",{attrs:{id:"实验对比"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#实验对比"}},[n._v("#")]),n._v(" 实验对比")]),n._v(" "),e("h3",{attrs:{id:"od方法的实验对比"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#od方法的实验对比"}},[n._v("#")]),n._v(" OD方法的实验对比")]),n._v(" "),e("p",[n._v("​\t\t在本节中，我们比较了PASCAL VOC 2007测试集和MS COCO验证集上不同OD方法在AR方面的性能。具体而言，我们比较了表1中PASCAL VOC 2007测试集上的七种方法，即BING[67]、OBJ[1]、EB[35]、GOP[30]、SS[29]、RPN[40]和MCG[86]。")]),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209223639215.png",alt:"image-20220209223639215"}})]),n._v(" "),e("p",[n._v("​\t\t在表2中MS COCO的验证集上的7种方法，MCG[86]、DeepMask[33]、DeepMaskZoom[33]、DeepMask2[34]、Sharp Mask[34]、SharpMaskZoom[34]和FastMask[36]。")]),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209223700500.png",alt:"image-20220209223700500"}})]),n._v(" "),e("p",[n._v("​\t\t这里，DeepMaskZoom表示将原始图像缩放到多个比例，然后在每个比例上应用DeepMask。同样的方法也用于ShapMaskZoom。DeepMask2基于39层ResNet[27]实现，并根据SharpMask中的网络架构修改了头部组件。DeepMask和Deep MaskZoom构建在VGG网络上[25]，而DeepMask2、SharpMask、SharpMaskZoom和FastMask构建在39层ResNet上[27]。")]),n._v(" "),e("p",[n._v("​\t\t在表1中，我们可以观察到，在不使用任何深度学习技术的人中，MCG是最好的OD方法。它甚至比RPN更好，RPN是建立在CNN上的一种OD方法。这主要是因为RPN设计的网络架构相对简单，它只是更快的R-CNN整个框架的一部分，其最终目标是COD而不是OD。我们可以在表2中观察到，许多基于深度学习模型的OD方法在很大程度上优于MCG（约5-13%），这主要是由于更强大的特征表示和更好的学习框架。更具体地说，在基于深度学习的OD方法中，除了具体设计的网络架构外，最终检测性能也与所采用的主干CNNModel高度相关。例如，使用ResNet作为主干CNN模型（例如DeepMask2和SharpMaskZoom）的OD方法通常比使用VGG网络（例如DeepMask和DeepMaskZoom）的OD方法的性能高出约2-6%。此外，通过比较表1和表2中MCG的性能，我们可以观察到MS COCO基准比PASCAL VOC 2007更具挑战性。在这一研究领域需要做出更大的努力，因为在这两个基准上的表现都有很大的改进空间。")]),n._v(" "),e("p",[n._v("​\t\t在表1和表2中，我们还报告了比较OD方法的运行时间。从这些表中，我们可以观察到，最快的OD方法是BING和RPN，它们可以在大约0.2秒的时间内处理每个图像。SS和MSG可以在不使用深度模型的情况下获得优于OD方法的操作。然而，与所有其他方法相比，它们的计算成本要大得多。基于深度模型的OD方法通常可以有效地进行测试，因为它们的计算成本约为每幅图像0.2-2秒。")]),n._v(" "),e("h3",{attrs:{id:"sod方法的实验对比"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sod方法的实验对比"}},[n._v("#")]),n._v(" SOD方法的实验对比")]),n._v(" "),e("p",[n._v("​\t\t在本节中，我们报告了一些具有代表性的基于DNN的SOD模型的定量比较结果。我们选择了2015年和2016年发布的七个模型，并发布了它们的代码或计算显著性图：LEGS[46]、DHS[20]、DCL[48]、ELD[47]、MCDL[87]、MDF[83]和RFCN[49]。ECSSD[811，PASCAL-S[70]和HKU-IS[83]数据集的实验结果见表3（就Fmeasure而言）、表4（就MAE而言）和图3（就PRC而言）。根据F和MAE的结果。DHS[20]是最好的显著性模型，而DCL[48]和RFCN[49]排在第二位。")]),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209224335826.png",alt:"image-20220209224335826"}})]),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209224352799.png",alt:"image-20220209224352799"}})]),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209224408864.png",alt:"image-20220209224408864"}})]),n._v(" "),e("p",[n._v("​\t\t在这些模型中，LEGS、MDF、MCDL和ELD基于提取的局部区域（即超像素或对象建议）。这些方法通常独立地估计每个局部区域的显著性得分。因此，它们在有效整合上下文信息方面存在局限性。因此，如表3所示，此类方法通常具有更多的计算成本，但它们无法获得优于其他方法的检测结果。相反，DHS，DCL。RFCN基于FCNs，可以同时对所有像素进行显著性推断。因此，如表3所示，它们通常非常快，尤其是对于DHS，它只需要一个forward，无需任何预处理或后处理。此外，由于FCNs以整个图像为输入，连续的深卷积层可以有效地合并大的上下文，这解释了这些方法获得更好的性能。")]),n._v(" "),e("p",[n._v("​\t\t为了深入了解SOD算法的性能，我们在最先进的显著性模型DHS上进行了消融实验[20]。与OD和COD任务不同，SOD任务通常有训练测试数据的标准分离，SOD任务没有这样的标准。尽管大多数SOD算法在MSRA-10K和DUT-OMRON数据集上实现了训练过程，在其他数据集上实现了测试过程，但当使用不同数量的训练数据时，它们会获得不同的性能。为此，我们首先分析了训练图像数量的影响。具体来说，我们从[20]中使用的原始训练集中（包含9500张图像）随机选择了三分之一和三分之二的图像，生成了两个子训练集，分别包含3167和6333张图像。通过使用这两个子训练集，我们获得了DHS-1/3和DHS-2/3的性能，如表4所示。从表4的前三行可以看出，随着训练图像的增加，检测性能会不断提高。这表明我们可以通过使用更多的训练图像来提高显著性模型的性能。值得一提的是，在将模型与其他现有显著性模型进行比较时，还应将其训练图像数保持在合理范围内（少于10000），以确保公平比较。")]),n._v(" "),e("p",[n._v("​\t\t然后，我们分析了主干CNN模型的影响。最初的DHS模型使用VGG 16层网络[25]作为主干模型。在这里，我们探索了另外两个更深层次的CNN网络，即ResNet50和ResNet101[27]，作为DHS模型的主干网络。注意，在原始的ALDHS模型中，作者使用了五步细化，将VGG特征映射从Conv4_3合并到Conv1_2，从而将显著性映射从28 x 28恢复到224 x 224。然而，ResNet网络的第一个卷积层都是跨步2，这意味着我们只能使用四步细化，并将显著性映射恢复到112 x 112。因此，我们还通过四个细化步骤（命名为DHS-RCL2）测试了原始DHS模型的结果，以进行公平比较。如表4中的最后四行所示，当使用相同的细化步骤时，更深的主干模型可以获得更好的最终性能。然而，由于最后一个细化步骤在提高最终性能方面起着非常重要的作用，因此使用ResNet作为主干模型的性能无法超过使用VGG网的性能。由于类似于DHS的网络结构在其他现代检测方法中被广泛使用，上述分析对于我们平衡ResNet带来的权衡非常重要。")]),n._v(" "),e("h3",{attrs:{id:"cod方法的实验对比"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#cod方法的实验对比"}},[n._v("#")]),n._v(" COD方法的实验对比")]),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209231603363.png",alt:"image-20220209231603363"}})]),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209231614762.png",alt:"image-20220209231614762"}})]),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209231631968.png",alt:"image-20220209231631968"}})]),n._v(" "),e("center",[e("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://myimgs.obs.cn-east-2.myhuaweicloud.com/Typora/image-20220209231647707.png",alt:"image-20220209231647707"}})]),n._v(" "),e("p",[n._v("​\t\t在本节中，我们选择了过去三年（2015-2017）发布的几种基于CNN的代表性目标检测方法，对PASCAL VOC2007数据集[79]、PASCAL VOC 2012数据集[16]和MS COCO数据集[80]进行性能比较。这些方法包括fast R-CNN[18]、faster R-CNN[40]、nside-Outside Net(ION) [63]、G-CNN[59]、SSD[58]、YOLO[57]和TOLOv2[60]。表5-7分别使用具有代表性的最新方法报告了PASCAL VOC 2007、PASCAL VOC 2012和COCO数据集的检测结果。表8显示了PASCAL VOC 2007上各种方法的准确性和速度比较。")]),n._v(" "),e("p",[n._v("​\t\t从表5-7可以看出，基于回归的COD方法，如SSD[58]和TOLOv2[60]，与基于对象建议的方法（如fast R-CNN[18]和faster R-CNN[40]）相比，可以获得更高的精度。SSD512[58]主要在所有三个基准（PASCAL VOC 2007、PASCAL VOC 2012和COCO数据集）上实现了最佳精度。表8所示的计算成本比较表明，基于回归的COD方法比基于对象建议的方法快得多，具有相当的精度。其中，YOLOv2是最具竞争力的实时检测器，也可以以不同的分辨率运行，以方便在速度和精度之间进行权衡。表5-8中的综合比较结果表明，基于回归的方法是一个有前途的COD方向。")]),n._v(" "),e("p",[n._v("​\t\t最近，大多数检测框架，如fast R-CNN[18]、faster R-CNN[40]和SSD[58]，都依赖VGGNet-16[25]作为主干CNN模型。尽管VGGNet-16是一个功能强大、准确的分类网络，但它的计算成本很高。VGGNet-16的卷积层需要对224 x 224像素的图像进行306.9亿次浮点运算。最近提出的YOLO[57]使用基于GoogleNet架构的定制网络[26]来提高检测速度。YOLO网络确实比基于VGG-Net-16的方法更快，仅使用85.2亿次运算来完成一次正向传球，但精确度略低于VGG-Net-16。为了进一步减少计算负担，同时提高精度，YOLOv2[60]提出了一种新的分类模型，名为Darknet-19，它只需要55.8亿次操作就可以处理图像。精度和速度的综合比较结果表明，通过设计更实用的模型结构（如YOLOv2[60]），基于回归的方法是一种很有前途的COD方向。")]),n._v(" "),e("h2",{attrs:{id:"讨论"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#讨论"}},[n._v("#")]),n._v(" 讨论")]),n._v(" "),e("h3",{attrs:{id:"深度学习带来的优势"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#深度学习带来的优势"}},[n._v("#")]),n._v(" 深度学习带来的优势")]),n._v(" "),e("p",[n._v("基于我们的分析，基于深度学习的目标检测器带来的优势可以总结为以下四个方面。")]),n._v(" "),e("h4",{attrs:{id:"强大的特征表示"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#强大的特征表示"}},[n._v("#")]),n._v(" 强大的特征表示")]),n._v(" "),e("p",[n._v("​\t\t开发基于深度学习的目标检测器的最重要原因之一是在学习过程中构建的强大特征表示。这在[10]中得到了明确的证明，Girshick等人使用广泛使用的HOG功能及其扩展版本之一，将他们提出的基于CNN的物体检测器的COD结果与DPM[54]基线进行了比较。实验结果表明，基于CNN的目标检测器在AP方面比DPM基线的检测器高出24.2%以上。此外，Girshick等人[10]从实验结果分析了所采用的CNN体系结构的不同网络层的表示能力，我们可以观察到，通过微调，较深层可以获得比较浅层更好的性能，这表明从CNN深层提取的特征表示可以有效地用于表示图像区域的信息语义。还值得一提的是，与传统的目标检测方法相比，即使是在仅使用整个网络6%参数的“pool5”层中提取的特征，也可以获得超过10%的性能增益（就AP而言）。在OD和SOD的研究中也可以找到类似的实验结果。例如，通过仅使用简单的CNN架构在其框架中构建功能，RPN[40]、DeepProposal[17]和DeepBox[2]已经可以明显优于使用传统功能表示的现有OD方法。在SOD中，[83]和[20]指出，如何构建真实、有意义的特征表示是SOD中最关键的问题之一，他们已经证明，通过设计合理的深度网络架构，可以有效地解决这个问题。")]),n._v(" "),e("h4",{attrs:{id:"端到端学习"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#端到端学习"}},[n._v("#")]),n._v(" 端到端学习")]),n._v(" "),e("p",[n._v("​\t\t基于深度学习的目标检测方法的另一个优势在于其端到端的学习框架。我们知道，传统的目标检测方法通常需要单独的计算块，例如特征提取和模式分类（例如[35]和[67]用于OD[88]和[89]用于SOD，以及[54]和[90]用于COD）。基于深度学习的方法（例如[17]和[36]用于OD，[20]和[87]用于SOD，以及[18]和[57]用于COD）仅通过一个统一的CNN模型就可以从原始输入图像中获得所需的对象检测结果。与传统方法相比，这种端到端的学习方式可以带来两个好处：")]),n._v(" "),e("ul",[e("li",[n._v("在传统的目标检测方法中，它可以大大降低从多个候选目标到每个计算块中选择最优方法的复杂性。")]),n._v(" "),e("li",[n._v("以这种端到端的方式学习可以根据学习目标确定整个模型的参数。")])]),n._v(" "),e("p",[n._v("与传统方法（用于设计手工制作的功能）相比，这种学习方式可以显著减少整个系统中有用信息的丢失。")]),n._v(" "),e("h4",{attrs:{id:"多阶段、多任务目标"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#多阶段、多任务目标"}},[n._v("#")]),n._v(" 多阶段、多任务目标")]),n._v(" "),e("p",[n._v("​\t\t得益于端到端学习范式，基于深度学习的现代目标检测方法可以在多个学习阶段灵活地涉及所需的学习目标和多个学习任务。例如，在OD中，FastMask[36]提出的深度网络包含语义特征提取和基于滑动窗口的建议生成的学习阶段。学习目标包括三个方面：置信度损失、分割损失和区域注意损失(confidence loss, segmentation loss,region attention loss)。在SOD和LEGS[46]中，包含两个级联学习阶段。第一个阶段是局部估计阶段，目标是学习局部有区别的小块特征(local discriminative patch features)。第二个阶段是全局搜索阶段，目标是利用全局显著性线索之间的复杂关系。DHS[20]也有两个学习阶段，第一个阶段是显著性推理，第二个阶段是细节渲染。在COD中，faster R-CNN[40]设计有两个级联学习阶段，用于学习提取建议目标区域，并分别识别每个提取的建议对象区域的对象类别。Fast R-CNN[18]采用多任务损失来同时学习特定类别的对象检测器和边界盒回归函数。与传统的目标检测方法相比，这种多阶段和多任务目标的最重要的优点是，相应的学习过程可以考虑所有设计的学习目标来确定检测网络的最优参数，而传统的方法只能通过考虑一个主要的学习目标，并在预处理或后处理阶段利用其他有用的因素来训练主要的目标检测模型。")]),n._v(" "),e("h4",{attrs:{id:"大规模学习和知识转移"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#大规模学习和知识转移"}},[n._v("#")]),n._v(" 大规模学习和知识转移")]),n._v(" "),e("p",[n._v("​\t\t与浅层结构的学习模型相比，深度学习模型的成功主要归功于大量隐藏的神经元，这通常会产生数百万个自由参数，而浅层结构的学习模型的参数较少，以避免过度拟合。因此，DNN通常需要大规模的训练数据来实现其完整的学习能力，这使得深度模型能够从训练数据中捕获比浅层模型更丰富的模式。除了知识挖掘能力之外，深度学习模型的另一个优点是可以方便地将学习到的知识转移到相关的任务或场景中。这主要是通过使用目标域中的数据微调源域(source domain)中预训练的深层模型来实现的。例如，最具代表性的基于深度学习的COD方法[10]在图像分类任务下采用了在ImageNet上预训练的CNN模型。实验结果表明，与之前的最新研究结果相比，直接使用这种网络已经可以获得明显的改善，这表明在大规模学习中捕获的模式可以强大到足以完成广泛的任务。此外，文献[10]中的实验结果表明，微调后的网络可以进一步获得显著的性能增益，这证明了深度学习模型简单而有效的知识转移能力。基于这种能力，大量现代目标检测方法[2]、[19]、[34]、[36]、[42]、[58]、[59]、[62]可以从简单但有效的预训练阶段中受益。")]),n._v(" "),e("h3",{attrs:{id:"未来研究方向"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#未来研究方向"}},[n._v("#")]),n._v(" 未来研究方向")]),n._v(" "),e("p",[n._v("尽管近年来基于深度学习的目标检测方法在这一研究领域取得了巨大的成功，但仍有一些具有挑战性但有趣的研究方向需要考虑。")]),n._v(" "),e("h4",{attrs:{id:"训练具有有限人类注释的目标检测器"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#训练具有有限人类注释的目标检测器"}},[n._v("#")]),n._v(" 训练具有有限人类注释的目标检测器")]),n._v(" "),e("p",[n._v("​\t\t尽管最近基于深度学习的目标检测方法取得了显著的性能提升，但在实际应用中，目标检测研究领域的问题仍然在很大程度上没有得到解决，因为这些方法大多严重依赖于空前巨大的(unparalleled and tremendous)人类标记训练数据。在这种情况下，人们需要花费大量的精力和时间在繁琐的数据标注上，以训练深层目标检测器。根据我们的统计数据，我们需要花费大约15秒（在LabelMe等辅助工具的帮助下）来绘制一个边界框注释，该注释可以正确地包含感兴趣的对象。考虑到这一点，可能需要手动注释数十万个训练图像，并且每个图像可能包含来自不同类别的多个对象。为了缓解这个问题，"),e("font",{attrs:{color:"red"}},[n._v("弱监督目标检测方法")]),n._v("[3]、[13]、[76]、[99]近年来受到了广泛关注。然而，所获得的性能仍然远远不能令人满意，它们只能达到相应的全监督目标检测方法所获得性能的50%。因此，仍需进一步努力解决这一问题。")],1),n._v(" "),e("h4",{attrs:{id:"不可见对象类别的检测"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#不可见对象类别的检测"}},[n._v("#")]),n._v(" 不可见对象类别的检测")]),n._v(" "),e("p",[n._v("​\t\t大多数现有的目标检测方法都是针对与训练集中的目标类别相同的图像进行评估的。然而，目标检测的最终目标是检测给定测试图像中任何可能类别的所有目标。本质上，在现实世界的应用程序中，我们对所有对象类别都缺乏足够的注释。广泛使用的PASCAL VOC和MS COCO基准仅分别包含20和80个对象类别，这远远不够。ILSVRC对象检测基准包含200个对象类别，但仍然不够。在许多类别没有边界框级注释的情况下，未来的一个方向是"),e("font",{attrs:{color:"red"}},[n._v("建立zero-shot learning-based的方案（用于目标检测），其中现有检测器和这些检测器之间的跨概念/类别映射(cross-concept/category mappings)的组合可以允许我们为看不见的种类构建对象检测器")]),n._v("。作为SOD的一个新兴分支，共现性(cosaliency)检测[92-[94]和事件显著性检测[957]方法也可能是检测未发生的对象/事件的可能方法，因为它们可以从包含共同发生但未知的对象/事件的任何给定图像/视频组中学习。")],1),n._v(" "),e("h4",{attrs:{id:"提高检测鲁棒性的新学习策略"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#提高检测鲁棒性的新学习策略"}},[n._v("#")]),n._v(" 提高检测鲁棒性的新学习策略")]),n._v(" "),e("p",[n._v("​\t\t未来的另一个方向是"),e("font",{attrs:{color:"red"}},[n._v("提高对使用不平衡数据或噪声数据训练的对象类别的检测鲁棒性")]),n._v("。这里的不平衡问题主要是指目标检测中不同类别样本数的长尾分布(long-tailed distribution)。长尾属性表示少数目标种类经常出现，而大多数其他种类很少出现的现象。例如，在PASCAL VOC和ImageNet对象检测数据集中，对象类别（如person）的样本比其他对象类别（如sheep）的样本多得多。一些分析和实证结果表明，"),e("font",{attrs:{color:"red"}},[n._v("样本较多的对象类别将主导学习对象检测器，导致样本较少的其他对象类别学习不足")]),n._v("。因此，针对这个问题的一个未来方向是"),e("font",{attrs:{color:"red"}},[n._v("建立新的学习方案，以便在不同的对象类别中使用更均匀分布的样本数进行学习")]),n._v("。得益于一些最新的生成性学习模型，如生成性对抗网络（GAN）。通过合成来自潜在噪声向量的可用数据，可以丰富“在尾部”的对象类别的样本。相反，大规模人工标注不可避免地会引入噪声标注，如标签缺失或错误标注。为了解决人类注释中的噪声问题，进一步的研究可以设计基于权重的学习机制（例如基于自配学习(self-paced learning)[94]、[96]和课程学习(curriculum learning)[97]、[98]的模型），当人类注释的学习的目标类别是噪声时，可以进一步提高的学习鲁棒性。")],1),n._v(" "),e("h4",{attrs:{id:"od、sod和cod的统一学习框架"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#od、sod和cod的统一学习框架"}},[n._v("#")]),n._v(" OD、SOD和COD的统一学习框架")]),n._v(" "),e("p",[n._v("​\t\t目前在目标检测领域的研究已经提出了一些有效的基于深度学习的框架，例如[40]和[41]，同时用于OD和COD。实验结果表明，通过联合优化OD和类别特定检测任务的网络参数，网络可以进一步探索这些任务之间的潜在关系，并捕获可从这两项任务中受益的常见信息模式。基本上，如前所述，OD、SOD和COD之间存在丰富的关系。因此，建立新的框架，尤其是基于深度学习的框架，以同时解决这三个方向上的常见问题，是非常有意义的。一种可能的方法是"),e("font",{attrs:{color:"red"}},[n._v("构建一个深度网络，将注意力建模、建议挖掘(proposal mining)和类别识别等模块结合到一个统一的学习框架中")]),n._v("。通过这种方式，三个任务之间共享的信息方式可以被模型捕获，这可以进一步提高每个任务的性能。")],1),n._v(" "),e("h4",{attrs:{id:"基于检测的高层视觉理解"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基于检测的高层视觉理解"}},[n._v("#")]),n._v(" 基于检测的高层视觉理解")]),n._v(" "),e("p",[n._v("​\t\t最近先进的目标检测技术的出现，促进了一些以前从未涉及过的更高层次的视觉理解任务的发展。这种任务的一个代表性例子是图像/视频字幕(image/video captioning)。此任务的基本目标是自动生成一个句子来描述任何给定图像/视频的内容。物体检测技术可以提供物体位置和类别的关键信息，用于解释图像/视频场景中的物体是什么，它们放在哪里，以及它们对交互物体做了什么。从本质上说，准确的目标检测是将视觉领域与语言领域联系起来的关键。沿着这条研究路线，仍然有许多未开发但有趣的基于检测的应用（更高级别的视觉理解任务），它们构成了未来研究方向的另一个分支。")]),n._v(" "),e("h2",{attrs:{id:"结论"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#结论"}},[n._v("#")]),n._v(" 结论")]),n._v(" "),e("p",[n._v("​\t\t在本文中，我们回顾了主要基于高级深度学习技术的目标检测的最新进展。具体而言，回顾了目标检测三个方向的OD、SOD和COD的现代方法、基准数据集和评估指标。我们全面分析了这些方向之间的关系，对深度学习的优势进行了深入的讨论，并提出了一些未来可能的方向。")]),n._v(" "),e("h2",{attrs:{id:"致谢"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#致谢"}},[n._v("#")]),n._v(" 致谢")]),n._v(" "),e("p",[n._v("Dingwen Zhang和Gong Chen是本文的通讯作者。")]),n._v(" "),e("h2",{attrs:{id:"作者"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#作者"}},[n._v("#")]),n._v(" 作者")]),n._v(" "),e("p",[n._v("​\t\tjunweihan（junweihan2010@gmail.com）分别于1999年、2001年和2003年获得了模式识别和智能系统的学士、硕士和博士学位，均来自中国西安西北工业大学，目前担任该校教授。2003至2010年间，他是南洋理工大学、香港大学、爱尔兰都柏林城市大学和邓迪大学的研究员。他的研究兴趣包括计算机视觉和大脑成像分析。他是IEEE《人机系统、神经计算、机器视觉与应用》杂志的副主编。")]),n._v(" "),e("p",[n._v("​\t\tDingwen Zhang（zdw2006yyy@mail.nwpu.edu.cn）于2012年在中国西安西北工业大学获得自动化学士学位，目前正在那里攻读博士学位。自2015年10月以来，他一直是宾夕法尼亚州匹兹堡卡内基梅隆大学的访问学者。他的研究兴趣包括计算机视觉和多媒体处理。")]),n._v(" "),e("p",[n._v("​\t\tGong Cheng（chenggong1119@gmail.com）于2007年在中国西安西安西安电子大学获得自动化学士学位，2010年和2013年分别在中国西安西北工业大学获得模式识别和机器智能硕士和博士学位；他目前是后者的副教授。他的主要研究兴趣是计算机视觉和目标检测。")]),n._v(" "),e("p",[n._v("​\t\tNian Liu（liunian228@gmail.com）分别于2012年和2015年在中国西安西北工业大学（NPU）获得自动化学士和硕士学位。他目前正在NPU自动化学院攻读博士学位。他的研究兴趣包括计算机视觉，重点是显著性检测和深度学习。")]),n._v(" "),e("p",[n._v("​\t\tDong Xu(dong.xu@sydney.edu.au)于2001、2005分别在合肥中国科技大学获得电子工程学士学位和博士学位。他目前是澳大利亚悉尼大学电气与信息工程学院的教授。他是IEEE高级成员和国际模式识别协会研究员。")]),n._v(" "),e("h2",{attrs:{id:"参考文献"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考文献"}},[n._v("#")]),n._v(" 参考文献")]),n._v(" "),e("p",[n._v("[1] B. Alexe, T. Deselaers, and V. Ferrari, “Measuring the objectness of image win-\ndows,” IEEE Trans. Pattern Anal. Machine Intell., vol. 34, no. 11, pp. 2189–2202, 2012")]),n._v(" "),e("p",[n._v("[2] W. Kuo, B. Hariharan, and J. Malik, “Deepbox: Learning objectness with con-\nvolutional networks,” in Proc. IEEE Int. Conf. Computer Vision, 2015, pp. 2479–\n2487.\n[3] T. Deselaers, B. Alexe, and V. Ferrari, “Weakly supervised localization and\nlearning with generic knowledge,” Int. J. Computer Vision, vol. 100, no. 3, pp. 275–\n293, 2012.\n[4] Y. Wu, J. Lim, and M.-H. Yang, “Object tracking benchmark,” IEEE Trans.\nPattern Anal. Machine Intell., vol. 37, no. 9, pp. 1834–1848, 2015.\n[5] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y. Shum, “Learning\nto detect a salient object,” IEEE Trans. Pattern Anal. Machine Intell., vol. 33, no. 2,\npp. 353–367, 2011.\n[6] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. S. Torr, and S.-M. Hu, “Global con-\ntrast based salient region detection,” IEEE Trans. Pattern Anal. Machine Intell.,\nvol. 37, no. 3, pp. 569–582, 2015.\n[7] A. Borji and L. Itti, “State-of-the-art in visual attention modeling,” IEEE Trans.\nPattern Anal. Machine Intell., vol. 35, no. 1, pp. 185–207, 2013.\n[8] X. Li, T. Uricchio, L. Ballan, M. Bertini, C. G. Snoek, and A. D. Bimbo,\n“Socializing the semantic gap: A comparative survey on image tag assignment,\nrefinement, and retrieval,” ACM Comput. Surveys, vol. 49, no. 1, pp. 14, 2016.\n[9] B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik, “Hypercolumns for object\nsegmentation and fine-grained localization,” in Proc. IEEE Conf. Computer Vision\nand Pattern Recognition, 2015, pp. 447–456.\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for\naccurate object detection and semantic segmentation,” in Proc. IEEE Conf.\nComputer Vision and Pattern Recognition, 2014, pp. 580–587.\n[11] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li, S. Yang, Z.\nWang, and C.-C. Loy, “Deepid-net: Deformable deep convolutional neural networks\nfor object detection,” in Proc. IEEE Conf. Computer Vision and Pattern\nRecognition, 2015, pp. 2403–2412.\n[12] J. Tighe, M. Niethammer, and S. Lazebnik, “Scene parsing with object\ninstance inference using regions and per-exemplar detectors,” Int. J. Computer\nVision, vol. 112, no. 2, pp. 150–171, 2015.\n[13] A. Prest, C. Schmid, and V. Ferrari, “Weakly supervised learning of interac-\ntions between humans and objects,” IEEE Trans. Pattern Anal. Machine Intell.,\nvol. 34, no. 3, pp. 601–614, 2012.\n[14] Y. LeCun, F. J. Huang, and L. Bottou, “Learning methods for generic object\nrecognition with invariance to pose and lighting,” in Proc. IEEE Conf. Computer\nVision and Pattern Recognition, 2004, pp. 97–104.\n[15] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object detec-\ntion,” in Proc. Advances in Neural Information Processing Systems, 2013, pp.\n2553–2561.\n[16] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn, and A.\nZisserman, “The Pascal visual object classes challenge: A retrospective,” Int. J.\nComputer Vision, vol. 111, no. 1, pp. 98–136, 2015.\n[17] A. Ghodrati, A. Diba, M. Pedersoli, T. Tuytelaars, and L. Van Gool,\n“Deepproposal: Hunting objects by cascading deep convolutional layers,” in Proc.\nIEEE Int. Conf. Computer Vision, 2015, pp. 2578–2586.\n[18] R. Girshick, “Fast R-CNN,” in Proc. IEEE Int. Conf. Computer Vision, 2015,\npp. 1440–1448.\n[19] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:\nUnified, real-time object detection,” arXiv Preprint, arXiv:1506.02640, 2015.\n[20] N. Liu and J. Han, “DHSNet: Deep hierarchical saliency network for salient\nobject detection,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition,\n2016, pp. 678–686.\n[21] K. Fukushima, “Neocognitron: A self-organizing neural network model for a\nmechanism of pattern recognition unaffected by shift in position,” Biol. Cybernet.,\nvol. 36, no. 4, pp. 193–202, 1980.\n[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,\nand L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,”\nNeural Comput., vol. 1, no. 4, pp. 541–551, 1989.\n[23] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no.\n7553, pp. 436–444, 2015.\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification with\ndeep convolutional neural networks,” in Proc. Advances in Neural Information\nProcessing Systems, 2012, pp. 1097–1105.\n[25] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-\nscale image recognition,” arXiv Preprint, arXiv:1409.1556, 2014.\n[26] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V.\nVanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in Proc. IEEE\nConf. Computer Vision and Pattern Recognition, 2015, pp. 1–9.\n[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recog-\nnition,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2016, pp.\n770–778.\n[28] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A\nlarge-scale hierarchical image database,” in Proc. IEEE Conf. Computer Vision and\nPattern Recognition, 2009, pp. 248–255.\n[29] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\n“Selective search for object recognition,” Int. J. Computer Vision, vol. 104, no. 2,\npp. 154–171, 2013.\n[30] P. Krähenbühl and V. Koltun, “Geodesic object proposals,” in Proc. European\nConf. Computer Vision, 2014, pp. 725–739.\n[31] L. Bazzani, A. Bergamo, D. Anguelov, and L. Torresani, “Self-taught object\nlocalization with deep networks,” in Proc. IEEE Winter Conf. Applications of\nComputer Vision, 2016, pp. 1–9.\n[32] X. Hou, and L. Zhang, “Saliency detection: A spectral residual approach,” in\nProc. IEEE Conf. Computer Vision and Pattern Recognition, 2007, pp. 1–8.\n[33] P. O. Pinheiro, R. Collobert, and P. Dollar, “Learning to segment object candi-\ndates,” in Proc. Advances in Neural Information Processing Systems, 2015, pp.\n1990–1998.\n[34] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár, “Learning to refine object\nsegments,” in Proc. European Conf. Computer Vision, 2016, pp. 75–91.\n[35] C. L. Zitnick, and P. Dollár, “Edge boxes: Locating object proposals from\nedges,” in Proc. European Conf. Computer Vision, 2014, pp. 391–405.\n[36] H. Hu, S. Lan, Y. Jiang, Z. Cao, and F. Sha, “FastMask: Segment multi-scale\nobject candidates in one shot,” arXiv Preprint, arXiv:1612.08843, 2016.\n[37] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable object detection\nusing deep neural networks,” in Proc. IEEE Conf. Computer Vision and Pattern\nRecognition, 2014, pp. 2147–2154.\n[38] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, “Scalable, high-\nquality object detection,” arXiv Preprint, arXiv:1412.1441, 2014.\n[39] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking the\ninception architecture for computer vision,” in Proc. IEEE Conf. Computer Vision\nand Pattern Recognition, 2016, pp. 2818–2826.\n[40] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time\nobject detection with region proposal networks,” in Proc. Advances in Neural\nInformation Processing Systems, 2015, pp. 91–99.\n[41] T. Kong, A. Yao, Y. Chen, and F. Sun, “HyperNet: Towards accurate region\nproposal generation and joint object detection,” in Proc. IEEE Conf. Computer\nVision and Pattern Recognition, 2016, pp. 845–853.\n[42] H. Li, Y. Liu, W. Ouyang, and X. Wang, “Zoom out-and-in network with recur-\nsive training for object proposal,” arXiv Preprint, arXiv:1702.05711, 2017.\n[43] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, “Salient object detection: A sur-\nvey,” arXiv Preprint, arXiv:1411.5878, 2014.\n[44] A. Borji, H. R. Tavakoli, D. N. Sihite, and L. Itti, “Analysis of scores, data sets,\nand models in visual saliency prediction,” in Proc. IEEE Int. Conf. Computer\nVision, 2013, pp. 921–928.\n[45] J. Han, D. Zhang, X. Hu, L. Guo, J. Ren, and F. Wu, “Background prior-based\nsalient object detection via deep reconstruction residual,” IEEE Trans. Circuits\nSystems Video Technol., vol. 25, no. 8, pp. 1309–1321, 2015.\n[46] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, “Deep networks for saliency detec-\ntion via local estimation and global search,” in Proc. IEEE Conf. Computer Vision\nand Pattern Recognition, 2015, pp. 3183–3192.\n[47] G. Lee, Y.-W. Tai, and J. Kim, “Deep saliency with encoded low level distance\nmap and high level features,” in Proc. IEEE Conf. Computer Vision and Pattern\nRecognition, 2016, pp. 660-668.\n[48] G. Li, and Y. Yu, “Deep contrast learning for salient object detection,” in Proc.\nIEEE Conf. Computer Vision and Pattern Recognition, 2016, pp. 660–668.")]),n._v(" "),e("p",[n._v("[49] L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan, “Saliency detection with\nrecurrent fully convolutional networks,” in Proc. European Conf. Computer Vision,\n2016, pp. 825–841.\n[50] J. Yang, and M.-H. Yang, “Top-down visual saliency via joint CRF and diction-\nary learning,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition,\n2012, pp. 2296–2303.\n[51] S. He, R. W. Lau, and Q. Yang, “Exemplar-driven top-down saliency detection\nvia deep association,” in Proc. IEEE Conf. Computer Vision and Pattern\nRecognition, 2016, pp. 5723–5732.\n[52] H. Cholakkal, J. Johnson, and D. Rajan, “Backtracking ScSPM image classifier\nfor weakly supervised top-down saliency,” in Proc. IEEE Conf. Computer Vision\nand Pattern Recognition, 2016, pp. 5278–5287.\n[53] J. Zhang, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff, “Top-down neural atten-\ntion by excitation backprop,” in Proc. European Conf. Computer Vision, 2016, pp.\n543–559.\n[54] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object\ndetection with discriminatively trained part-based models,” IEEE Trans. Pattern\nAnal. Machine Intell., vol. 32, no. 9, pp. 1627–1645, 2010.\n[55] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep convo-\nlutional networks for visual recognition,” IEEE Trans. Pattern Anal. Machine\nIntell., vol. 37, no. 9, pp. 1904–1916, 2015.\n[56] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun,\n“Overfeat: Integrated recognition, localization and detection using convolutional net-\nworks,” arXiv Preprint, arXiv:1312.6229, 2013.\n[57] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:\nUnified, real-time object detection,” in Proc. IEEE Conf. Computer Vision and\nPattern Recognition, 2016, pp. 779–788.\n[58] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,\n“SSD: Single shot multibox detector,” in Proc. European Conf. Computer Vision,\n2016, pp. 21–37.\n[59] M. Najibi, M. Rastegari, and L. S. Davis, “G-CNN: An iterative grid based\nobject detector,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition,\n2016, pp. 2369–2377.\n[60] J. Redmon and A. Farhadi, “YOLO9000: Better, faster, stronger,” arXiv\nPreprint, arXiv:1612.08242, 2016.\n[61] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based object\ndetectors with online hard example mining,” in Proc. IEEE Conf. Computer Vision\nand Pattern Recognition, 2016, pp. 761–769.\n[62] G. Cheng, P. Zhou, and J. Han, “RIFD-CNN: Rotation-invariant and fisher dis-\ncriminative convolutional neural networks for object detection,” in Proc. IEEE\nConf. Computer Vision and Pattern Recognition, 2016, pp. 2884–2893.\n[63] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, “Inside-outside net:\nDetecting objects in context with skip pooling and recurrent neural networks,” in\nProc. IEEE Conf. Computer Vision and Pattern Recognition, 2016, pp. 2874–\n2883.\n[64] S. Gidaris, and N. Komodakis, “Object detection via a multi-region and seman-\ntic segmentation-aware CNN model,” in Proc. IEEE Int. Conf. Computer Vision,\n2015, pp. 1134–1142.\n[65] A. Shrivastava, and A. Gupta, “Contextual priming and feedback for faster\nR-CNN,” in Proc. European Conf. Computer Vision, 2016, pp. 330–348.\n[66] J. Feng, Y. Wei, L. Tao, C. Zhang, and J. Sun, “Salient object detection by\ncomposition,” in Proc. IEEE Int. Conf. Computer Vision, 2011, pp. 1028–1035.\n[67] M.-M. Cheng, Z. Zhang, W.-Y. Lin, and P. Torr, “BING: Binarized normed\ngradients for objectness estimation at 300fps,” in Proc. IEEE Conf. Computer\nVision and Pattern Recognition, 2014, pp. 3286–3293.\n[68] K.-Y. Chang, T.-L. Liu, H.-T. Chen, and S.-H. Lai, “Fusing generic objectness\nand visual saliency for salient object detection,” in Proc. IEEE Int. Conf. Computer\nVision, 2011, pp. 914–921.\n[69] P. Jiang, H. Ling, J. Yu, and J. Peng, “Salient region detection by UFO:\nUniqueness, focusness and objectness,” in Proc. IEEE Int. Conf. Computer Vision,\n2013, pp. 1976–1983.\n[70] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, “The secrets of salient\nobject segmentation,” in Proc. IEEE Conf. Computer Vision and Pattern\nRecognition, 2014, pp. 280–287.\n[71] T. Malisiewicz, A. Gupta, and A. A. Efros, “Ensemble of exemplar-svms for\nobject detection and beyond,” in Proc. IEEE Int. Conf. Computer Vision, 2011, pp.\n89–96.\n[72] M. Bar, “The proactive brain: Using analogies and associations to generate pre-\ndictions,” Trends Cogn. Sci., vol. 11, no. 7, pp. 280–289, 2007.\n[73] R. M. Nosofsky, “Attention, similarity, and the identification–categorization\nrelationship,” J. Exp. Psychol., vol. 115, no. 1, pp. 39, 1986.\n[74] A. Borji, “Boosting bottom-up and top-down visual features for saliency esti-\nmation,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2012, pp.\n438–445.\n[75] T. Judd, K. Ehinger, F. Durand, and A. Torralba, “Learning to predict where\nhumans look,” in Proc. IEEE Int. Conf. Computer Vision, 2009, pp. 2106–2113.\n[76] D. Zhang, D. Meng, L. Zhao, and J. Han, “Bridging saliency detection to weak-\nly supervised object detection based on self-paced curriculum learning,” in Proc.\nInt. Joint Conf. Artificial Intelligence, 2016, pp. 3538–3544.\n[77] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning deep\nfeatures for discriminative localization,” in Proc. IEEE Conf. Computer Vision and\nPattern Recognition, 2016, pp. 2921–2929.\n[78] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,\n“The pascal visual object classes (VOC) challenge,” Int. J. Computer Vision, vol.\n88, no. 2, pp. 303–338, 2010.\n[79] M. Everingham, A. Zisserman, C. K. Williams, L. Van Gool, M. Allan, C. M.\nBishop, O. Chapelle, N. Dalal, T. Deselaers, and G. Dorkó, “The PASCAL visual\nobject classes challenge 2007 (VOC2007) results,” 2007. [Online]. Available: http://\nhost.robots.ox.ac.uk/pascal/VOC/voc2007/workshop/index.html\n[80] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,\nand C. L. Zitnick, “Microsoft COCO: Common objects in context,” in Proc.\nEuropean Conf. Computer Vision, 2014, pp. 740–755.\n[81] Q. Yan, L. Xu, J. Shi, and J. Jia, “Hierarchical saliency detection,” in Proc.\nIEEE Conf. Computer Vision and Pattern Recognition, 2013, pp. 1155–1162.\n[82] V. Movahedi and J. H. Elder, “Design and perceptual validation of perfor-\nmance measures for salient object segmentation,” in Proc. IEEE Computer\nSociety Conf. Computer Vision and Pattern Recognition Workshops, 2010, pp.\n49–56.\n[83] G. Li and Y. Yu, “Visual saliency based on multiscale deep features,” in Proc.\nIEEE Conf. Computer Vision and Pattern Recognition, 2015, pp. 5455–5463.\n[84] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang, “Saliency detection via\ngraph-based manifold ranking,” in Proc. IEEE Conf. Computer Vision and Pattern\nRecognition, 2013, pp. 3166–3173.\n[85] M.-M. Cheng, N. J. Mitra, X. Huang, and S.-M. Hu, “Salientshape: Group\nsaliency in image collections,” Visual Computer, vol. 30, no. 4, pp. 443–453,\n2014.\n[86] J. Pont-Tuset, P. Arbelaez, J. T. Barron, F. Marques, and J. Malik,\n“Multiscale combinatorial grouping for image segmentation and object proposal\ngeneration,” IEEE Trans. Pattern Anal. Machine Intell., vol. 39, no. 1, pp. 128–\n140, 2017.\n[87] R. Zhao, W. Ouyang, H. Li, and X. Wang, “Saliency detection by multi-context\ndeep learning,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition,\n2015, pp. 1265–1274.\n[88] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li, “Salient object detec-\ntion: A discriminative regional feature integration approach,” in Proc. IEEE Conf.\nComputer Vision and Pattern Recognition, 2013, pp. 2083–2090.\n[89] P. Mehrani and O. Veksler, “Saliency segmentation based on learning and\ngraph cut refinement,” in Proc. British Machine Vision Conf., 2010, pp. 1–12.\n[90] P. Dollár, R. Appel, S. Belongie, and P. Perona, “Fast feature pyramids for\nobject detection,” IEEE Trans. Pattern Anal. Machine Intell., vol. 36, no. 8, pp.\n1532–1545, 2014.\n[91] Q. Hou, M.-M. Cheng, X. Hu, Z. Tu, and A. Borji, “Deeply supervised salient\nobject detection with short connections,” in Proc. IEEE Conf. Computer Vision and\nPattern Recognition, 2017, pp. 3203–3212.\n[92] D. Zhang, J. Han, J. Han, and L. Shao, “Cosaliency detection based on intrasa-\nliency prior transfer and deep intersaliency mining,” IEEE Trans. Neural Networks\nLearning Syst., vol. 27, no. 6, pp. 1163–1176, 2016.\n[93] D. Zhang, J. Han, C. Li, J. Wang, and X. Li, “Detection of co-salient objects by\nlooking deep and wide,” Int. J. Computer Vision, vol. 120, no. 2, pp. 215–232,\n2016.\n[94] D. Zhang, D. Meng, and J. Han, “Co-saliency detection via a self-paced multi-\nple-instance learning framework,” IEEE Trans. Pattern Anal. Machine Intell., vol.\n39, no. 5, pp. 865–878, 2017.\n[95] D. Zhang, J. Han, L. Jiang, S. Ye, and X. Chang, “Revealing event saliency in\nunconstrained video collection,” IEEE Trans. Image Processing, vol. 26, no. 4, pp.\n1746–1758, 2017.\n[96] D. Zhang, L. Yang, D. Meng, D. Xu, and J. Han, “SPFTN: A self-paced fine-\ntuning network for segmenting objects in weakly labelled videos,” in Proc. IEEE\nConf. Computer Vision and Pattern Recognition, 2017, pp. 4429–4437.\n[97] D. Zhang, J. Han, Y. Yang, and D. Huang, “Learning category-specific 3D\nshape models from weakly labeled 2D images,” in Proc. IEEE Conf. Computer\nVision and Pattern Recognition, 2017, pp. 4573–4581.\n[98] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”\nin Proc. Annu. Int. Conf. Machine Learning, 2009, pp. 41–48.\n[99] Y. Yuan, X. Liang, X. Wang, D.-Y. Yeung, and A. Gupta, “Temporal dynamic\ngraph LSTM for action-driven video object detection,” arXiv Preprint,\narXiv:1708.00666, 2017.")])],1)}),[],!1,null,null,null);t.default=o.exports}}]);